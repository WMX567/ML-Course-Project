{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits # The MNIST data set is in scikit learn data set\n",
    "from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n",
    "from sklearn.metrics import accuracy_score # \n",
    "import numpy as np\n",
    "import numpy.random as r # We will randomly initialize our weights\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# load dataset\n",
    "digits=load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# scalar dataset\n",
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)\n",
    "\n",
    "#split original dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create binary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule label:0-4 count as 0, label:5-9 count as 1\n",
    "y_binary = np.zeros(len(y))\n",
    "for i in range(len(y)):\n",
    "    if  y[i] == 0 or y[i] == 1 or y[i] == 2 or y[i] == 3 or y[i] == 4:\n",
    "        y_binary[i] = 0\n",
    "    else:\n",
    "        y_binary[i] =1\n",
    "y= np.ravel(y)\n",
    "\n",
    "#Split the binary data set\n",
    "X_binary_train, X_binary_test, y_binary_train, y_binary_test = train_test_split(X, y_binary, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data set for muti classification of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original train set is X:(1078, 64), y:(1078,)\n",
      "We create 9 trainning data set according to class 0-9.\n",
      "The new train set is New y List: (10, 1078)\n",
      "The original y is: [9 4 2 ... 1 2 3]\n",
      "The 0 class: y:[[0. 0. 0. ... 0. 0. 0.]]\n",
      "The 1 class: y:[[0. 0. 0. ... 1. 0. 0.]]\n",
      "The 2 class: y:[[0. 0. 1. ... 0. 1. 0.]]\n",
      "The 3 class: y:[[0. 0. 0. ... 0. 0. 1.]]\n",
      "The 4 class: y:[[0. 1. 0. ... 0. 0. 0.]]\n",
      "The 5 class: y:[[0. 0. 0. ... 0. 0. 0.]]\n",
      "The 6 class: y:[[0. 0. 0. ... 0. 0. 0.]]\n",
      "The 7 class: y:[[0. 0. 0. ... 0. 0. 0.]]\n",
      "The 8 class: y:[[0. 0. 0. ... 0. 0. 0.]]\n",
      "The 9 class: y:[[1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "new_y_train_list =[] # 0-9\n",
    "\n",
    "for j in range(10):\n",
    "    new_y = np.zeros(len(y_train))\n",
    "    for i in range(len(y_train)):\n",
    "        if  y_train[i] == j:\n",
    "            new_y[i] = 1\n",
    "        else:\n",
    "            new_y[i] = 0\n",
    "    new_y_train_list.append(new_y)\n",
    "new_y_train_list = np.mat(new_y_train_list)\n",
    "print(\"The original train set is X:{}, y:{}\".format(X_train.shape,y_train.shape))\n",
    "print(\"We create 9 trainning data set according to class 0-9.\")\n",
    "print(\"The new train set is New y List: {}\".format(new_y_train_list.shape))\n",
    "print(\"The original y is: {}\".format(y_train))\n",
    "print(\"The 0 class: y:{}\".format(new_y_train_list[0]))\n",
    "print(\"The 1 class: y:{}\".format(new_y_train_list[1]))\n",
    "print(\"The 2 class: y:{}\".format(new_y_train_list[2]))\n",
    "print(\"The 3 class: y:{}\".format(new_y_train_list[3]))\n",
    "print(\"The 4 class: y:{}\".format(new_y_train_list[4]))\n",
    "print(\"The 5 class: y:{}\".format(new_y_train_list[5]))\n",
    "print(\"The 6 class: y:{}\".format(new_y_train_list[6]))\n",
    "print(\"The 7 class: y:{}\".format(new_y_train_list[7]))\n",
    "print(\"The 8 class: y:{}\".format(new_y_train_list[8]))\n",
    "print(\"The 9 class: y:{}\".format(new_y_train_list[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation in Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None: \n",
      "0.9360222531293463\n",
      "L2:\n",
      "0.9499304589707928\n"
     ]
    }
   ],
   "source": [
    "# experiment in Sklearn:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(penalty ='none',class_weight= 'balanced', random_state=0, solver='newton-cg', multi_class='ovr')\n",
    "logreg.fit(X_train, y_train)\n",
    "prepro = logreg.predict_proba(X_test)\n",
    "acc = logreg.score(X_test,y_test)\n",
    "print(\"None: \")\n",
    "print (acc)\n",
    "logreg = LogisticRegression(penalty ='l2',class_weight= 'balanced', random_state=0, solver='newton-cg', multi_class='ovr')\n",
    "logreg.fit(X_train, y_train)\n",
    "prepro = logreg.predict_proba(X_test)\n",
    "acc = logreg.score(X_test,y_test)\n",
    "print (\"L2:\")\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our implementation of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Some params\n",
    "learning_rate = 0.1\n",
    "num_iters = 3000 # The number of iteratins to run the gradient ascent algorithm\n",
    "\n",
    "# logistc regression\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# Initialize parameters w\n",
    "w = np.zeros((X_train.shape[1], 1))\n",
    "\n",
    "def hypothesis(X , w):\n",
    "    return 1/(1+np.exp(-np.dot(X,w)))\n",
    "yhat = hypothesis(X_train, w)\n",
    "\n",
    "def log_likelihood(X ,y , w ):\n",
    "    hx = hypothesis(X , w)\n",
    "    log_likelihood =0\n",
    "    for i in range(X.shape[0]):\n",
    "        if y[i] == 0:\n",
    "            if hx[i] ==1:\n",
    "                continue\n",
    "            log_likelihood = log_likelihood + np.log(1-hx[i])\n",
    "        else: \n",
    "            if hx[i] ==0:\n",
    "                continue\n",
    "            log_likelihood = log_likelihood + np.log(hx[i])\n",
    "    return log_likelihood\n",
    "\n",
    "def Logistic_Regresion_Gradient_Ascent(X, y, learning_rate, num_iters):\n",
    "    #initialize\n",
    "    log_likelihood_values = []\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    N = X.shape[0] \n",
    "    #do iteration\n",
    "    for i in range(num_iters):\n",
    "        gradient = np.dot(X.transpose(),(y-hypothesis(X,w)))\n",
    "        w = w + (learning_rate/N)*gradient\n",
    "        if (i % 100) == 0:\n",
    "            log_likelihood_values.append(log_likelihood(X,y,w))  \n",
    "    return w, log_likelihood_values\n",
    "\n",
    "def ridge_log_likelihood(X , y, w, C = 0.65):    \n",
    "    hx = hypothesis(X , w)\n",
    "    log_likelihood = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        if y[i] == 0:\n",
    "            if hx[i] ==1:\n",
    "                continue     \n",
    "            log_likelihood = log_likelihood + np.log(1-hx[i])\n",
    "        else: \n",
    "            if hx[i] ==0:\n",
    "                continue\n",
    "            log_likelihood = log_likelihood + np.log(hx[i])\n",
    "    reg_term = C*np.dot(w.T,w)\n",
    "    log_likelihood = log_likelihood  - reg_term\n",
    "    return log_likelihood\n",
    "\n",
    "def Ridge_Regresion_Gradient_Ascent(X, y, learning_rate, num_iters, C = 0.65):\n",
    "    #initialize\n",
    "    ridge_log_likelihood_values = []\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    N = X.shape[0] \n",
    "    #do iteration\n",
    "    for i in range(num_iters):\n",
    "        gradient = np.dot(X.transpose(),(y - hypothesis(X,w)))\n",
    "        w = w + (learning_rate/N)*gradient - learning_rate*C*w \n",
    "        if (i % 100) == 0:\n",
    "            ridge_log_likelihood_values.append(ridge_log_likelihood(X, y, w, C))  \n",
    "    return w, ridge_log_likelihood_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build multiclass model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train n model (0,9)\n",
    "w =[]\n",
    "log_likelihood_values =[]\n",
    "for i in range(10):\n",
    "    w_new, log_likelihood_values_new = Ridge_Regresion_Gradient_Ascent(X_train, new_y_train_list[i].transpose(), learning_rate, num_iters, 0.65)\n",
    "    w.append(w_new)\n",
    "    log_likelihood_values.append(log_likelihood_values_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with ridge multiclass regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision is: 0.8803894297635605\n"
     ]
    }
   ],
   "source": [
    "#pretdict\n",
    "result = []\n",
    "for i in range(len(y_test)):\n",
    "    hx_every_example = []\n",
    "    for j in range(len(w)):\n",
    "        hx = hypothesis(X_test[i],w[j])\n",
    "        hx_every_example.append(np.linalg.det(hx))\n",
    "    predict_class =np.argmax(hx_every_example)\n",
    "    result.append(predict_class)\n",
    "    \n",
    "#caculate precision\n",
    "right = 0\n",
    "for i in range(len(y_test)):\n",
    "    if result[i] == y_test[i]:\n",
    "        right = right + 1\n",
    "                    \n",
    "print(\"The precision is: {}\".format(right/len(y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In ridge regression,C = 0.6, the precision is: 0.8803894297635605\n",
      "In ridge regression,C = 0.65, the precision is: 0.8803894297635605\n",
      "In ridge regression,C = 0.7, the precision is: 0.8803894297635605\n",
      "In ridge regression,C = 0.75, the precision is: 0.8803894297635605\n",
      "In ridge regression,C = 0.8, the precision is: 0.8803894297635605\n"
     ]
    }
   ],
   "source": [
    "# to find best C\n",
    "for C in [0.6, 0.65, 0.7, 0.75, 0.8]:\n",
    "    # train n model (0,9)\n",
    "    w =[]\n",
    "    log_likelihood_values =[]\n",
    "    for i in range(10):\n",
    "        w_new, log_likelihood_values_new = Ridge_Regresion_Gradient_Ascent(X_train, new_y_train_list[i].transpose(), learning_rate, num_iters, C)\n",
    "        w.append(w_new)\n",
    "        log_likelihood_values.append(log_likelihood_values_new)\n",
    "    #pretdict\n",
    "    result = []\n",
    "    for i in range(len(y_test)):\n",
    "        hx_every_example = []\n",
    "        for j in range(len(w)):\n",
    "            hx = hypothesis(X_test[i],w[j])\n",
    "            hx_every_example.append(np.linalg.det(hx))\n",
    "        predict_class =np.argmax(hx_every_example)\n",
    "        result.append(predict_class)\n",
    "    \n",
    "    #caculate precision\n",
    "    right = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if result[i] == y_test[i]:\n",
    "            right = right + 1\n",
    "                    \n",
    "    print(\"In ridge regression,C = {}, the precision is: {}\".format(C, right/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with logistic multiclass regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logistic regression precision is: 0.8706536856745479\n"
     ]
    }
   ],
   "source": [
    "# the accuracy of original one\n",
    "# train n model (0,9)\n",
    "w =[]\n",
    "#log_likelihood_values =[]\n",
    "for i in range(10):\n",
    "    w_new, log_likelihood_values_new = Logistic_Regresion_Gradient_Ascent(X_train, new_y_train_list[i].transpose(), learning_rate, num_iters)\n",
    "    w.append(w_new)\n",
    "\n",
    "#pretdict\n",
    "result = []\n",
    "for i in range(len(y_test)):\n",
    "    hx_every_example = []\n",
    "    for j in range(len(w)):\n",
    "        hx = hypothesis(X_test[i],w[j])\n",
    "        hx_every_example.append(np.linalg.det(hx))\n",
    "    predict_class =np.argmax(hx_every_example)\n",
    "    result.append(predict_class)\n",
    "    \n",
    "#caculate precision\n",
    "right = 0\n",
    "for i in range(len(y_test)):\n",
    "    if result[i] == y_test[i]:\n",
    "        right = right + 1\n",
    "                    \n",
    "print(\"The logistic regression precision is: {}\".format(right/len(y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on another dataset\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "data = data.drop(labels=['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "data = data.dropna()\n",
    "data_dummy = pd.get_dummies(data[['Sex', 'Embarked']])\n",
    "data_conti = pd.DataFrame(data, columns=['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], index=data.index)\n",
    "data = data_conti.join(data_dummy)\n",
    "\n",
    "#split data into X and y\n",
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# standard\n",
    "stdsc = StandardScaler()\n",
    "X_train_conti_std = stdsc.fit_transform(X_train[['Age', 'SibSp', 'Parch', 'Fare']])\n",
    "X_test_conti_std = stdsc.fit_transform(X_test[['Age', 'SibSp', 'Parch', 'Fare']])\n",
    "\n",
    "# change ndarray into dataframe\n",
    "X_train_conti_std = pd.DataFrame(data=X_train_conti_std, columns=['Age', 'SibSp', 'Parch', 'Fare'], index=X_train.index)\n",
    "X_test_conti_std = pd.DataFrame(data=X_test_conti_std, columns=['Age', 'SibSp', 'Parch', 'Fare'], index=X_test.index)\n",
    "\n",
    "# Pclass is an ordered categorical variable\n",
    "X_train_cat = X_train[['Pclass']]\n",
    "X_test_cat = X_test[['Pclass']]\n",
    "#   disordered encoded categorical variable\n",
    "X_train_dummy = X_train[['Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']]\n",
    "X_test_dummy = X_test[['Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']]\n",
    "\n",
    "# linked them to the dataframe\n",
    "X_train_set = [X_train_cat, X_train_conti_std, X_train_dummy]\n",
    "X_test_set = [X_test_cat, X_test_conti_std, X_test_dummy]\n",
    "X_train = pd.concat(X_train_set, axis=1)\n",
    "X_test = pd.concat(X_test_set, axis=1)\n",
    "\n",
    "#change back into the ndarray\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "y_train = np.mat(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logistic regression precision is: 0.7616822429906542\n",
      "C = 0.04, The Ridge regression precision is: 0.7710280373831776\n",
      "C = 0.05, The Ridge regression precision is: 0.7757009345794392\n",
      "C = 0.06, The Ridge regression precision is: 0.7710280373831776\n",
      "C = 0.03, The Ridge regression precision is: 0.7710280373831776\n"
     ]
    }
   ],
   "source": [
    "# Some params\n",
    "learning_rate = 0.1\n",
    "num_iters = 3000 # The number of iteratins to run the gradient ascent algorithm\n",
    "\n",
    "# train logistic regression\n",
    "w1, log_likelihood_values1 = Logistic_Regresion_Gradient_Ascent(X_train, y_train.transpose(), learning_rate, num_iters)\n",
    "#pretdict\n",
    "result = []\n",
    "for i in range(len(y_test)):\n",
    "    hx = hypothesis(X_test[i],w1)\n",
    "    if hx > 0.5:\n",
    "        result.append(1)\n",
    "    else:\n",
    "        result.append(0)\n",
    "    \n",
    "#caculate precision\n",
    "right = 0\n",
    "for i in range(len(y_test)):\n",
    "    if result[i] == y_test[i]:\n",
    "        right = right + 1\n",
    "                    \n",
    "print(\"The logistic regression precision is: {}\".format(right/len(y_test)))\n",
    "\n",
    "\n",
    "\n",
    "# train ridge regression\n",
    "for C in [0.04, 0.05,0.06, 0.03]:\n",
    "    w2, log_likelihood_values2= Ridge_Regresion_Gradient_Ascent(X_train, y_train.transpose(), learning_rate, num_iters, C)\n",
    "    #pretdict\n",
    "    result = []\n",
    "    for i in range(len(y_test)):\n",
    "        hx = hypothesis(X_test[i],w2)\n",
    "        if hx > 0.5:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "\n",
    "    #caculate precision\n",
    "    right = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if result[i] == y_test[i]:\n",
    "            right = right + 1\n",
    "\n",
    "    print(\"C = {}, The Ridge regression precision is: {}\".format(C, right/len(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
